{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import socket\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.conf.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k8sMaster = \"https://10.0.0.1:443\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Azure storage access info\n",
    "blob_account_name = \"azureopendatastorage\"\n",
    "blob_container_name = \"censusdatacontainer\"\n",
    "blob_relative_path = \"release/us_population_zip/\"\n",
    "blob_sas_token = r\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow SPARK to read from Blob remotely\n",
    "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
    "print('Remote blob path: ' + wasbs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = conf.setMaster(\"k8s://\" + k8sMaster) \\\n",
    "    .setAppName(\"pyspark-notebook\") \\\n",
    "    .set(\"spark.kubernetes.executor.container.image\", \"docker.io/guislander/spark-py:latest\")  \\\n",
    "    .set(\"spark.kubernetes.namespace\", \"jhub\") \\\n",
    "    .set(\"spark.kubernetes.authenticate.driver.serviceAccountName\",\"spark\")  \\\n",
    "    .set(\"spark.executor.instances\", \"2\") \\\n",
    "    .set(\"spark.kubernetes.executor.request.cores\",\"100m\") \\\n",
    "    .set(\"spark.kubernetes.executor.limit.cores\",\"1\") \\\n",
    "    .set(\"spark.kubernetes.pyspark.pythonVersion\",\"3\") \\\n",
    "    .set(\"fs.azure.sas.%s.%s.blob.core.windows.net\" % (blob_container_name, blob_account_name),\n",
    "  blob_sas_token) \\\n",
    "    .set(\"spark.driver.host\", socket.gethostbyaddr(socket.gethostname())[2][0]) \\\n",
    "    .set(\"spark.driver.port\", \"29413\") \\\n",
    "    .set(\"spark.driver.pod.name\", socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# SPARK read parquet, note that it won't load any data yet by now\n",
    "df = spark.read.parquet(wasbs_path)\n",
    "print('Register the DataFrame as a SQL temporary view: source')\n",
    "df.createOrReplaceTempView('source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 10 rows\n",
    "print('Displaying top 10 rows: ')\n",
    "display(spark.sql('SELECT * FROM source LIMIT 10'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "LoadAzureBlobParquet",
  "notebookId": 3308813946069514
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
